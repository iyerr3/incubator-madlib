# coding=utf-8

"""
@file mlp_igd.py_in

@brief Multilayer perceptron using IGD: Driver functions

@namespace mlp_igd
"""
import plpy

from utilities.control import MinWarning
from utilities.utilities import py_list_to_sql_string
from utilities.utilities import extract_keyvalue_params
from utilities.validate_args import output_tbl_valid
from utilities.validate_args import input_tbl_valid
from utilities.validate_args import is_var_valid
from utilities.validate_args import table_exists
from utilities.validate_args import get_expr_type
from utilities.validate_args import array_col_has_same_dimension
from utilities.validate_args import array_col_has_no_null
from utilities.validate_args import array_col_dimension
from utilities.utilities import _assert


def mlp(schema_madlib, source_table, output_table, independent_varname,
        dependent_varname, hidden_layer_sizes,
        optimizer_param_str, activation, is_classification, **kwargs):
    """
    Args:
        @param schema_madlib
        @param source_table
        @param output_table
        @param independent_varname
        @param dependent_varname
        @param hidden_layer_sizes
        @param optimizer_param_str

    Returns:
        None
    """
    with MinWarning('info'):

        optimizer_params = _get_optimizer_params(optimizer_param_str or "")
        _validate_args(source_table, output_table, independent_varname,
        dependent_varname, hidden_layer_sizes,
        optimizer_params, is_classification)

        n_iterations = 1
        prev_state = None
        tolerance = optimizer_params["tolerance"]
        activation_name = _get_activation_function_name(activation)
        activation_index = _get_activation_index(activation_name)
        num_input_nodes = array_col_dimension(source_table,independent_varname)
        num_output_nodes = 0

        if(is_classification):
            dependent_variable_sql = """
            SELECT DISTINCT {dependent_varname} FROM {source_table};
            """.format(dependent_varname=dependent_varname,
                       source_table=source_table)
            labels = plpy.execute(dependent_variable_sql)
            one_hot_dependent_varname='ARRAY['
            num_output_nodes = len(labels)
            for x in labels:
                label = _format_label(x[dependent_varname])
                one_hot_dependent_varname+=dependent_varname+"="+str(label)+","
            # Remove the last comma
            one_hot_dependent_varname = one_hot_dependent_varname[:-1]
            one_hot_dependent_varname+=']::integer[]'
            dependent_varname=one_hot_dependent_varname
        if(not is_classification):
            expr_type = get_expr_type(dependent_varname,source_table)
            if "[]" not in expr_type:
                dependent_varname = "ARRAY["+dependent_varname+"]"
            num_output_nodes = array_col_dimension(source_table,dependent_varname)
        while True:
            if prev_state:
                prev_state_str = py_list_to_sql_string(prev_state, array_type="double precision")
            else:
                layer_sizes = [num_input_nodes] + hidden_layer_sizes + [num_output_nodes]
                prev_state_str = "(NULL)::DOUBLE PRECISION[]"
            train_sql = """
                SELECT
                    {schema_madlib}.mlp_igd_step(
                        ({independent_varname})::DOUBLE PRECISION[],
                        ({dependent_varname})::DOUBLE PRECISION[],
                        {prev_state},
                        {layer_sizes},
                        ({step_size})::FLOAT8,
                        {activation},
                        {is_classification}) as curr_state
                FROM {source_table} AS _src""".format(
                schema_madlib=schema_madlib,
                independent_varname=independent_varname,
                dependent_varname=dependent_varname,
                prev_state=prev_state_str,
                layer_sizes=py_list_to_sql_string(layer_sizes, array_type="integer"),
                step_size=optimizer_params["step_size"],
                source_table=source_table,
                activation=activation_index,
                is_classification=int(is_classification))
            curr_state = plpy.execute(train_sql)[0]["curr_state"]
            dist_sql = """
                SELECT {schema_madlib}.internal_mlp_igd_distance(
                        {prev_state},
                        {curr_state}) as state_dist
                """.format(schema_madlib=schema_madlib,
                           prev_state=prev_state_str,
                           curr_state=py_list_to_sql_string(curr_state, "double precision"),
                           tolerance=tolerance)
            state_dist = plpy.execute(dist_sql)[0]["state_dist"]
            if ((state_dist and state_dist < tolerance) or
                    n_iterations > optimizer_params["n_iterations"]):
                break
            prev_state = curr_state
            n_iterations += 1
        _build_output_tables(schema_madlib, output_table, curr_state, n_iterations)
# ----------------------------------------------------------------------


def _build_output_tables(schema_madlib, output_table, final_state, n_iterations):
    final_state_str = py_list_to_sql_string(final_state, array_type="double precision")

    model_table_query = """
        CREATE TABLE {output_table} AS
            SELECT
                (result).coeff AS coeff,
                (result).loss           AS loss,
                {n_iterations}                       AS num_iterations
                -- (result).num_rows_processed     AS num_rows_processed,
                -- n_tuples_including_nulls - (result).num_rows_processed
            FROM
            (
                SELECT
                    {schema_madlib}.internal_mlp_igd_result(
                        {final_state_str}
                    ) AS result
            ) rel_state_subq
        """.format(**locals())
    plpy.execute(model_table_query)
# ----------------------------------------------------------------------


def _get_optimizer_params(param_str):
    params_defaults = {
        "step_size": (0.001, float),
        "n_iterations": (100, int),
        "n_tries": (1, int),
        "tolerance": (0.001, float),
    }
    param_defaults = dict([(k, v[0]) for k, v in params_defaults.items()])
    param_types = dict([(k, v[1]) for k, v in params_defaults.items()])

    if not param_str:
        return param_defaults

    name_value = extract_keyvalue_params(param_str, param_types, param_defaults,
                                         ignore_invalid=True)
    return name_value
# ----------------------------------------------------------------------

def _validate_args_classification(source_table, output_table, independent_varname,
        dependent_varname, hidden_layer_sizes,
        optimizer_params):
    expr_type = get_expr_type(dependent_varname,source_table)
    int_types = ['integer', 'smallint', 'bigint']
    text_types = ['text', 'varchar', 'character varying', 'char', 'character']
    boolean_types = ['boolean']
    _assert("[]" in expr_type or expr_type in int_types+text_types+boolean_types,"Dependent variable column should refer to an integer, boolean, text, varchar, or character type.")
# ----------------------------------------------------------------------

def _validate_args_regression(source_table, output_table, independent_varname,
        dependent_varname, hidden_layer_sizes,
        optimizer_params):
    expr_type = get_expr_type(dependent_varname,source_table)
    int_types = ['integer', 'smallint', 'bigint']
    float_types = ['double precision','real']
    _assert("[]" in expr_type or expr_type in int_types+float_types,"Dependent variable column should refer to an array or numeric type")
    if("[]" in expr_type):
        _assert(array_col_has_same_dimension(source_table,dependent_varname),"Dependent variable column should refer to arrays of the same length")

# ----------------------------------------------------------------------


def _validate_args(source_table, output_table, independent_varname,
        dependent_varname, hidden_layer_sizes,
        optimizer_params, is_classification):
    """
    Args:
        @param kwargs

    Returns:

    """
    input_tbl_valid(source_table, "MLP")
    output_tbl_valid(output_table, "MLP")
    _assert(is_var_valid(source_table, independent_varname),
            "MLP error: invalid independent_varname "
            "('{independent_varname}') for source_table "
            "({source_table})!".format(independent_varname=independent_varname,
                                       source_table=source_table))

    _assert(is_var_valid(source_table, dependent_varname),
            "MLP error: invalid dependent_varname "
            "('{dependent_varname}') for source_table "
            "({source_table})!".format(dependent_varname=dependent_varname,
                                       source_table=source_table))


    _assert(hidden_layer_sizes is not None,"hidden_layer_sizes may not be null")
    _assert(type(hidden_layer_sizes) is list,"hidden_layer_sizes must be an array of integers")
    _assert(all(type(value) is int for value in hidden_layer_sizes), "MLP error: Hidden layers sizes must be integers")
    _assert(all(value >= 0 for value in hidden_layer_sizes),"MLP error: Hidden layers sizes must be greater than 0.")
    _assert(optimizer_params["tolerance"]>=0,"MLP error: Tolerance should be greater than or equal to 0.")
    _assert(optimizer_params["n_tries"]>=1,"MLP error: Number of tries should be greater than or equal to 1")
    _assert(optimizer_params["n_iterations"]>=1,"MLP error: Number of iterations should be greater than or equal to 1")
    _assert(optimizer_params["step_size"]>0,"MLP error: Stepsize should be greater than 0.")
    _assert("[]" in get_expr_type(independent_varname,source_table),"Independent variable column should refer to an array")
    _assert(array_col_has_same_dimension(source_table,independent_varname),"Independent variable column should refer to arrays of the same length")
    _assert(array_col_has_no_null(source_table,independent_varname),"Independent variable column should refer to arrays of the same length")

    if is_classification:
        _validate_args_classification(source_table, output_table, independent_varname,
        dependent_varname, hidden_layer_sizes,
        optimizer_params)
    else:
        _validate_args_regression(source_table, output_table, independent_varname,
        dependent_varname, hidden_layer_sizes,
        optimizer_params)
# ----------------------------------------------------------------------




def _get_activation_function_name(activation_function):
    if not activation_function:
        activation_function = 'sigmoid'
    else:
        # Add non-linear kernels below after implementing them.
        supported_activation_function = ['sigmoid', 'tanh', 'relu']
        try:
            # allow user to specify a prefix substring of
            # supported kernels. This works because the supported
            # kernels have unique prefixes.
            activation_function = next(x for x in supported_activation_function
                    if x.startswith(activation_function))
        except StopIteration:
            # next() returns a StopIteration if no element found
            plpy.error("SVM Error: Invalid activation function: "
                       "{0}. Supported activation functions are ({1})"
                       .format(activation_function, ','.join(sorted(supported_activation_function))))
    return activation_function
# ------------------------------------------------------------------------------

def _get_activation_index(activation_name):
    table = {"relu":0,"sigmoid":1,"tanh":2}
    return table[activation_name]

def _format_label(label):
    if type(label) is str:
        return "'" + label + "'"
    return label

def mlp_help(schema_madlib, message, is_classification):
    method = 'mlp_classification' if is_classification else 'mlp_regression'
    #TODO
    int_types = ['integer', 'smallint', 'bigint']
    text_types = ['text', 'varchar', 'character varying', 'char', 'character']
    boolean_types = ['boolean']
    supported_types = " "*33+", ".join(text_types)+"\n"+" "*33+", ".join(int_types+boolean_types)
    label_description_classification="Name of a column which specifies label.\n"+" "*33+ "Supported types are:\n"+supported_types
    label_description_regression="Dependent variable. May be an array for multiple\n"+\
        " "*33+"regression or the name of a column which is any\n"+" "*33+"numeric type for single regression"
    label_description = label_description_classification if is_classification else label_description_regression
    args = dict(schema_madlib=schema_madlib, method=method,label_description=label_description)

    summary = """
    ----------------------------------------------------------------
                            SUMMARY
    ----------------------------------------------------------------
    Multi Layer Perceptron (MLP) is a model for regression and
    classification

    Also called "vanilla neural networks", they consist of several
    fully connected hidden layers with non-linear activation
    functions.

    For more details on function usage:
        SELECT {schema_madlib}.{method}('usage')

    For a small example on using the function:
        SELECT {schema_madlib}.{method}('example')""".format(**args)

    usage = """
    ---------------------------------------------------------------------------
                                    USAGE
    ---------------------------------------------------------------------------
    SELECT {schema_madlib}.{method}(
        source_table,         -- name of input table
        output_table,         -- name of output model table
        independent_varname,  -- name of independent variable
        dependent_varname,    -- {label_description}
        hidden_layer_sizes,   -- Array of integers indicating the
                                 number of hidden units per layer.
                                 Length equal to the number of hidden layers.
        optimizer_params,     -- optional, default NULL
                                 parameters for optimization in
                                 a comma-separated string of key-value pairs.

            step_size DOUBLE PRECISION, -- Default: 0.001
                                           Learning rate
            n_iterations INTEGER,       -- Default: 100
                                           Number of iterations per try
            n_tries INTEGER,            -- Default: 1
                                           Total number of training cycles,
                                           with random initializations to avoid
                                           local minima.
            tolerance DOUBLE PRECISION, -- Default: 0.001
                                           If the distance in loss between
                                           two iterations is less than the
                                           tolerance training will stop, even if
                                           n_iterations has not been reached

        activation            -- optional, default: 'sigmoid'.
                                 supported activations: 'relu', 'sigmoid',
                                 and 'tanh'
    );


    ---------------------------------------------------------------------------
                                    OUTPUT
    ---------------------------------------------------------------------------
    The model table produced by svm contains the following columns:

    #TODO
    """.format(**args)

    regression_example = """
    - Create input table

    CREATE TABLE lin_housing_wi (id serial, x float8[], grp_by_col int, y float8);
    COPY lin_housing_wi (x, grp_by_col, y) FROM STDIN NULL '?' DELIMITER '|';
    {1,0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,396.90,4.98} | 1 | 24.00
    {1,0.02731,0.00,7.070,0,0.4690,6.4210,78.90,4.9671,2,242.0,17.80,396.90,9.14} | 1 | 21.60
    {1,0.02729,0.00,7.070,0,0.4690,7.1850,61.10,4.9671,2,242.0,17.80,392.83,4.03} | 1 | 34.70
    {1,0.03237,0.00,2.180,0,0.4580,6.9980,45.80,6.0622,3,222.0,18.70,394.63,2.94} | 1 | 33.40
    {1,0.06905,0.00,2.180,0,0.4580,7.1470,54.20,6.0622,3,222.0,18.70,396.90,5.33} | 1 | 36.20
    {1,0.02985,0.00,2.180,0,0.4580,6.4300,58.70,6.0622,3,222.0,18.70,394.12,5.21} | 1 | 28.70
    {1,0.08829,12.50,7.870,0,0.5240,6.0120,66.60,5.5605,5,311.0,15.20,395.60,12.43} | 1 | 22.90
    {1,0.14455,12.50,7.870,0,0.5240,6.1720,96.10,5.9505,5,311.0,15.20,396.90,19.15} | 1 | 27.10
    {1,0.21124,12.50,7.870,0,0.5240,5.6310,100.00,6.0821,5,311.0,15.20,386.63,29.93} | 1 | 16.50
    {1,0.17004,12.50,7.870,0,0.5240,6.0040,85.90,6.5921,5,311.0,15.20,386.71,17.10} | 1 | 18.90
    {1,0.22489,12.50,7.870,0,0.5240,6.3770,94.30,6.3467,5,311.0,15.20,392.52,20.45} | 1 | 15.00
    {1,0.11747,12.50,7.870,0,0.5240,6.0090,82.90,6.2267,5,311.0,15.20,396.90,13.27} | 1 | 18.90
    {1,0.09378,12.50,7.870,0,0.5240,5.8890,39.00,5.4509,5,311.0,15.20,390.50,15.71} | 1 | 21.70
    \.

    - Generate a multilayer perception with a two hidden layers of 5 units
    each. Use the x column as the independent variables, and use the class
    column as the classification. Set the tolerance to 0 so that 300
    iterations will be run. Use a sigmoid activation function.
    The model will be written to mlp_regress_result.

    SELECT mlp_regression(
        'lin_housing_wi',           -- Source table
        'mlp_regress_result',  -- Desination table
        'x',                        -- Independent variable
        'y',                        -- Dependent variable
        ARRAY[5,5],                 -- Number of hidden units per layer
        'step_size=0.007,
        n_iterations=300,
        tolerance=0',
        'sigmoid');                 -- Activation

    """

    classification_example = """
    - Create input table

    CREATE TABLE iris_data(
        id integer,
        attributes numeric[],
        class_text varchar,
        class integer
    );

    INSERT INTO iris_data VALUES
    (1,ARRAY[5.1,3.5,1.4,0.2],'Iris-setosa',1),
    (2,ARRAY[4.9,3.0,1.4,0.2],'Iris-setosa',1),
    (3,ARRAY[4.7,3.2,1.3,0.2],'Iris-setosa',1),
    (4,ARRAY[4.6,3.1,1.5,0.2],'Iris-setosa',1),
    (5,ARRAY[5.0,3.6,1.4,0.2],'Iris-setosa',1),
    (6,ARRAY[5.4,3.9,1.7,0.4],'Iris-setosa',1),
    (7,ARRAY[4.6,3.4,1.4,0.3],'Iris-setosa',1),
    (8,ARRAY[5.0,3.4,1.5,0.2],'Iris-setosa',1),
    (9,ARRAY[4.4,2.9,1.4,0.2],'Iris-setosa',1),
    (10,ARRAY[4.9,3.1,1.5,0.1],'Iris-setosa',1),
    (11,ARRAY[7.0,3.2,4.7,1.4],'Iris-versicolor',2),
    (12,ARRAY[6.4,3.2,4.5,1.5],'Iris-versicolor',2),
    (13,ARRAY[6.9,3.1,4.9,1.5],'Iris-versicolor',2),
    (14,ARRAY[5.5,2.3,4.0,1.3],'Iris-versicolor',2),
    (15,ARRAY[6.5,2.8,4.6,1.5],'Iris-versicolor',2),
    (16,ARRAY[5.7,2.8,4.5,1.3],'Iris-versicolor',2),
    (17,ARRAY[6.3,3.3,4.7,1.6],'Iris-versicolor',2),
    (18,ARRAY[4.9,2.4,3.3,1.0],'Iris-versicolor',2),
    (19,ARRAY[6.6,2.9,4.6,1.3],'Iris-versicolor',2),
    (20,ARRAY[5.2,2.7,3.9,1.4],'Iris-versicolor',2),


    - Generate a multilayer perception with a single hidden layer of 5 units.
    Use the attributes column as the independent variables, and use the class
    column as the classification. Set the tolerance to 0 so that 1000
    iterations will be run. Use a hyperbolic tangent activation function.
    The model will be written to mlp_result.

    SELECT mlp_classification(
        'iris_data',      -- Source table
        'mlp_result',     -- Destination table
        'attributes',     -- Input features
        'class',          -- Label
        ARRAY[5],         -- Number of units per layer
        'step_size=0.001,
        n_iterations=1000,
        tolerance=0',
        'tanh');          -- Activation function

    """
    example = classification_example if is_classification else regression_example
    if not message:
        return summary
    elif message.lower() in ('usage', 'help', '?'):
        return usage
    elif message.lower() == 'example':
        return example
    else:
        return """
            No such option. Use "SELECT {schema_madlib}.{method}()" for help.
        """.format(**args)

